{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityaadarsh99/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.cross_validation import train_test_split # for spliting dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer # bow-->1gram and 2 gram\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # tf-idf\n",
    "from gensim.models import Word2Vec  # w2v\n",
    "from gensim.models import KeyedVectors # to understanding w2v using google pre trained model\n",
    "from sklearn.metrics import accuracy_score # to check the accuracy of model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score # k-fold cv\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '#' sign to download data-set\n",
    "#!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3575.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-datasets/18/2157/Reviews.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1539455264&Signature=sUXQz0kX2395nqKOQppw23fSVbEPjFPhNtI1TC5FN1fOhx8RBGKLP1d1UZgiftoI1XI3TLxlFu6JwY%2BL7LwhxM46VQp89iJ%2BJZ8PaaY%2F61q8y%2BVchuDT8v4UnmbJ5%2Bkvf77HaNiJrAcqSY1K0C66npHhhaAgMzmvHJtTOnpZ70LFbZ6g1X%2Bh%2Ba2Tmkuiae%2F3CVIjnkE7sO2X4l3o5x4H45gu3EWzyrlqWJDwx0EJmDbnsOCsCzIEie7in70HwkDOwxGq9WdCGscFlFVDfd8W%2BV00yUfT82%2F2%2F4H4WGrnvI0uxFECct3b0n1F%2BbACM0sO0gvmmXAZNgbeOIpsUAclRw%3D%3D\" -O \"Reviews.csv.zip\" -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import zipfile\\ndata=zipfile.ZipFile(\"Reviews.csv.zip\")\\ndata.extractall()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove '#' sign to unzip the dataset\n",
    "\"\"\"import zipfile\n",
    "data=zipfile.ZipFile(\"Reviews.csv.zip\")\n",
    "data.extractall()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1229d7a0a060fa28d9af279edbb18e392f44ade"
   },
   "source": [
    ">   # Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4eb4898dd48098f8467ac0842ccf7031c2605dc8"
   },
   "outputs": [],
   "source": [
    "#loading the amazon dataset\n",
    "dataset=pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b055326cd0060297d256f6fbae459a416e9f1377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f115e029b492a93acade9b6bd568620e86245067"
   },
   "source": [
    "> # preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e20017e38dbc2e0162ad8c0e967ef4cbb185f982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (568454, 10)\n",
      "after removing duplicate values-->shape = (393141, 10)\n"
     ]
    }
   ],
   "source": [
    "# sorting the value\n",
    "dataset.sort_values(by='Id',inplace=True )\n",
    "#finding the dublicate values using 'df.dublicated'\n",
    "dataset[dataset.duplicated(subset={'ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'})].shape\n",
    "#alternate way to drop dublicate values\n",
    "dataset_no_dup=dataset.drop_duplicates(subset={'ProfileName','Score','Time','Summary'},keep='first')\n",
    "print(f\"before {dataset.shape}\")\n",
    "print(f\"after removing duplicate values-->shape = {dataset_no_dup.shape}\")\n",
    "# %age of no. of review reamin in data set\n",
    "(dataset_no_dup.size/dataset.size)*100\n",
    "\n",
    "# removing reviews where \"HelpfulnessNumerator>HelpfulnessDenominator\"\n",
    "dataset_no_dup=dataset_no_dup[dataset_no_dup['HelpfulnessNumerator']<=dataset_no_dup['HelpfulnessDenominator']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "8f32067d78d1791819e62ed3d5c2cf627a47718f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363393, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking reviews whose score is not equal to 3\n",
    "filtered_dataset=dataset_no_dup[dataset_no_dup['Score']!=3]\n",
    "filtered_dataset.shape\n",
    "#creating a function to filter the reviews (if score>3 --> positive , if score<3 --> negative)\n",
    "def partition(x):\n",
    "    if x>3:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "score=filtered_dataset['Score']\n",
    "pos_neg=score.map(partition)\n",
    "filtered_dataset['Score']=pos_neg\n",
    "print(filtered_dataset.shape)\n",
    "filtered_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the format of timestamp to ('%Y-%m-%d %H:%M:%S') and sorting dataset based on time\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "time=[]\n",
    "for timestamp in filtered_dataset['Time']:\n",
    "    t=datetime.datetime.fromtimestamp(timestamp).strftime(('%Y-%m-%d %H:%M:%S'))\n",
    "    time.append(t)\n",
    "filtered_dataset['time']=time   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by time\n",
    "filtered_dataset.sort_values(by='time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4bfc6dd18200a439fe4480be0cfabe6f8486020d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/adityaadarsh99/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sno = nltk.stem.SnowballStemmer('english')#snowball stemmer\n",
    "\n",
    "stop=set(stopwords.words('english')) #set of stopwords\n",
    "\n",
    "#clean html tags\n",
    "def cleanhtml(sent):\n",
    "    pattern=re.compile(r'<.*?>')\n",
    "    cleansent=re.sub(pattern,\" \",sent)\n",
    "    return cleansent\n",
    "\n",
    "#cleean punctuation\n",
    "def cleanpunc(word):\n",
    "    clean_punc=re.sub(r'[?|!|\\'|\"|#]',' ',word)\n",
    "    clean_punc=re.sub(r'[.|,|)|(|\\|/]',' ',clean_punc)\n",
    "    return clean_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c8c1b53bc92bc4ea60aa1bec6083212c077c7420"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "x=0 # number of reviews you want to clean/pre-processed\n",
    "positive_words=[]\n",
    "negative_words=[]\n",
    "str1=''\n",
    "final_sent=[] # storing the list of final pre-processed sentences\n",
    "i=0\n",
    "\n",
    "for sent in filtered_dataset[\"Text\"]:\n",
    "    sent=cleanhtml(sent) #removing html tags\n",
    "    filtered_sentence=[]\n",
    "    for w in sent.split():\n",
    "        for clean_words in cleanpunc(w).split():\n",
    "            if((len(clean_words)>2) & (clean_words.isalpha())):\n",
    "                if (clean_words.lower() not in stop):\n",
    "                    s=(sno.stem(clean_words.lower()))\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (filtered_dataset['Score'].values[i]=='positive'):\n",
    "                        positive_words.append(s)\n",
    "                    if (filtered_dataset['Score'].values[i]=='negative'):\n",
    "                        negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1=\" \".join(filtered_sentence) #str of all the cleaned words\n",
    "    final_sent.append(str1) # appending cleaned words to sentence\n",
    "    i=i+1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "01c437345ee8684a056bfbdb16664663b639a5cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363393\n",
      "\n",
      "Before cleaning :\n",
      " I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
      "\n",
      "After cleaning :\n",
      " get crazi realli imposs today find french vhs version film could pleas tell someth tks\n"
     ]
    }
   ],
   "source": [
    "print(len(final_sent))\n",
    "print(\"\\nBefore cleaning :\\n\",filtered_dataset[\"Text\"][10])\n",
    "print(f\"\\nAfter cleaning :\\n {final_sent[10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6c50e0f9ff51327712924638560bf98819297155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363393, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>time</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150523</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>1999-10-08 03:00:00</td>\n",
       "      <td>witti littl book make son laugh loud recit car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150500</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>1999-10-25 03:00:00</td>\n",
       "      <td>rememb see show air televis year ago child sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451855</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>1999-12-02 03:00:00</td>\n",
       "      <td>beetlejuic well written movi everyth excel act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230284</th>\n",
       "      <td>230285</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>1999-12-06 03:00:00</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451854</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>2000-01-03 03:00:00</td>\n",
       "      <td>beetlejuic excel funni movi keaton hilari wack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "150523  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "150500  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "451855  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "230284  230285  B00004RYGX  A344SMIA5JECGM           Vincent P. Ross   \n",
       "451854  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score       Time  \\\n",
       "150523                     0                       0  positive  939340800   \n",
       "150500                     2                       2  positive  940809600   \n",
       "451855                     0                       0  positive  944092800   \n",
       "230284                     1                       2  positive  944438400   \n",
       "451854                     0                       0  positive  946857600   \n",
       "\n",
       "                                                  Summary  \\\n",
       "150523                          EVERY book is educational   \n",
       "150500  This whole series is great way to spend time w...   \n",
       "451855                               Entertainingl Funny!   \n",
       "230284                            A modern day fairy tale   \n",
       "451854                                         FANTASTIC!   \n",
       "\n",
       "                                                     Text  \\\n",
       "150523  this witty little book makes my son laugh at l...   \n",
       "150500  I can remember seeing the show when it aired o...   \n",
       "451855  Beetlejuice is a well written movie ..... ever...   \n",
       "230284  A twist of rumplestiskin captured on film, sta...   \n",
       "451854  Beetlejuice is an excellent and funny movie. K...   \n",
       "\n",
       "                       time                                       cleaned_text  \n",
       "150523  1999-10-08 03:00:00  witti littl book make son laugh loud recit car...  \n",
       "150500  1999-10-25 03:00:00  rememb see show air televis year ago child sis...  \n",
       "451855  1999-12-02 03:00:00  beetlejuic well written movi everyth excel act...  \n",
       "230284  1999-12-06 03:00:00  twist rumplestiskin captur film star michael k...  \n",
       "451854  2000-01-03 03:00:00  beetlejuic excel funni movi keaton hilari wack...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset['cleaned_text']=final_sent\n",
    "print(filtered_dataset.shape)\n",
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "daafb4a4f3218f63d4596644b540980c4961a66e"
   },
   "source": [
    "## 1. Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "a05a5efa7ab7ef69c3476586940e1aab8957951c"
   },
   "outputs": [],
   "source": [
    "# one gram-BOW from \"sklean.feature_extraction.text.CountVectorizer\"\n",
    "count_vect=CountVectorizer()\n",
    "bow_cleaned_text=count_vect.fit_transform(filtered_dataset['cleaned_text'])\n",
    "print(\"bow_cleaned_text\",bow_cleaned_text.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityaadarsh99/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/adityaadarsh99/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "''# visualising unigram BOW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sc=StandardScaler(with_mean=False)\n",
    "a=sc.fit_transform(bow_cleaned_text[:4000])\n",
    "a=a.todense()\n",
    "\n",
    "tsne=TSNE(n_components=2 , perplexity=30.0 , n_iter=1000 , learning_rate=200.0 )\n",
    "tsne_data=tsne.fit_transform(a)\n",
    "\n",
    "# creating a new data frame which help us in ploting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, filtered_dataset['Text'][:1000])).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"dim1\", \"dim2\", \"score\"))\n",
    "\n",
    "# Ploting the result of tsne\n",
    "sns.FacetGrid(tsne_df, hue=\"score\", size=6).map(plt.scatter, 'dim1', 'dim2').add_legend()\n",
    "plt.title(\"TSNE for Bag of Words\")\n",
    "plt.show()''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9a2b7dcc0480bf64e0b527da5f71c5f3d97f81c1"
   },
   "outputs": [],
   "source": [
    "# two gram_BOW \n",
    "count_vect_gram=CountVectorizer(ngram_range=(1,2))\n",
    "bow_cleaned_text_2gram=count_vect_gram.fit_transform(filtered_dataset['cleaned_text'])\n",
    "print(\"bow_cleaned_text_2gram\",bow_cleaned_text_2gram.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "895599c90ea40038d951fadc252b90af84ea22c7"
   },
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "0209112be7402ec016daf6d1d4ca9c80742b2f7d"
   },
   "outputs": [],
   "source": [
    "# tf-idf \"from sklearn.feature_extraction.text.TfidfVectorizer\"\n",
    "tf_idf=TfidfVectorizer()\n",
    "tf_idf_cleaned_text=tf_idf.fit_transform(filtered_dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "a52baad81651bb79c8a55b16553dd15bb4223fbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363393, 70047)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_cleaned_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "b1ea1e4325da85524ea236db7fe389471603dbd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (363393, 2881737)\n",
      "the number of unique words including both unigrams and bigrams  2881737\n"
     ]
    }
   ],
   "source": [
    "# tf-idf 2-gram --> increasing no. of gram can increase the no. of dimension drastically\n",
    "tf_idf_2gram = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf_cleaned_text_2gram = tf_idf_2gram.fit_transform(filtered_dataset['cleaned_text'].values)\n",
    "print(\"the type of count vectorizer \",type(tf_idf_cleaned_text_2gram))\n",
    "print(\"the shape of out text TFIDF vectorizer \",tf_idf_cleaned_text_2gram.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", tf_idf_cleaned_text_2gram.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8e6302b0ce67e34896439a6c8b4d2f942e69798"
   },
   "source": [
    "## 3. avg word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "745243999b23a07fc0ac01b719cdba3366dfebe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# understanding w2v using goolgle trained model of 300 dimension\\n# w2v lib from \"gensim.models import KeyedVectors\"\\nfrom gensim.models import KeyedVectors\\ngoogle_w2v=KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\",encoding=\\'utf8\\',binary=True)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# understanding w2v using goolgle trained model of 300 dimension\n",
    "# w2v lib from \"gensim.models import KeyedVectors\"\n",
    "from gensim.models import KeyedVectors\n",
    "google_w2v=KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\",encoding='utf8',binary=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "c90234ec181dd4152d191b718b1e8a253c033a34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"google_w2v.distance('woman','queen')\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"google_w2v.distance('woman','queen')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "0acac0932b93cd9ca55bb22356ffa81017462271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting our text-->vector using w2v with 50-dim\n",
    "# more the dimension of each word = better the semantic of word\n",
    "# using lib from \"gensim.models.Word2Vec\"\n",
    "# to run w2v we need list of list of the words as w2v covert each world into number of dim\n",
    "\n",
    "list_of_sent=[]\n",
    "for sent in filtered_dataset['cleaned_text'].values:\n",
    "    list_of_sent.append((str(sent)).split())\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50)\n",
    "# vocablary of w2v model of amazon dataset\n",
    "vocab=w2v_model.wv.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "7e909e82711f558ec52e3529ca37f2620e86d4b7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityaadarsh99/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \n",
      "/home/adityaadarsh99/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('men', 0.801190972328186),\n",
       " ('hypothyroid', 0.7887751460075378),\n",
       " ('reproduct', 0.7742730379104614),\n",
       " ('surgeon', 0.7650048136711121),\n",
       " ('profess', 0.7603486180305481),\n",
       " ('alzheim', 0.7558683156967163),\n",
       " ('cardiac', 0.7451004981994629),\n",
       " ('fighter', 0.7446705102920532),\n",
       " ('lactat', 0.7424254417419434),\n",
       " ('practition', 0.7419098615646362)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding w2v on amzon fine food reviews dataset\n",
    "w2v_model.similar_by_word('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbdfcfaff39a6189234848490463a74cfd91b063",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# w2v representaion of word \"women\" in 50-dim\n",
    "w2v_model.wv['women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f4208f81461a7a563e316507552d8196bb555611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "-->procedure to make avg w2v of each reviews\n",
    "    1. find the w2v of each word\n",
    "    2. sum-up w2v of each word in a sentence\n",
    "    3. divide the total w2v of sentence by total no. of words in the sentence\n",
    "'''\n",
    "\n",
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "\n",
    "for sent in list_of_sent[:20000]: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in vocab:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d2d31b65134574de9a68a4a87f8251c6da3d5eb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39404453, -0.17196729,  0.69991714,  0.8254756 , -0.09350525,\n",
       "       -0.64485487, -0.29970349, -0.21707925,  0.60134434, -0.50405598,\n",
       "        0.21142184,  0.00867293, -0.50363094, -0.11216523, -0.33022491,\n",
       "       -0.08943456,  0.30571758,  0.14359926, -0.39849741, -0.27793845,\n",
       "        0.34446185, -1.10546325, -0.34677354,  0.08652229, -0.15224074,\n",
       "        0.89145936, -0.21042541,  0.68806242,  0.54342073, -0.30826682,\n",
       "        0.07178857,  0.357291  , -0.1069258 , -0.20938604,  0.33380609,\n",
       "       -0.35579884, -0.25072395,  0.23776049,  0.361678  , -0.21168045,\n",
       "        0.64264491,  0.38354636, -0.10461359,  0.10568834,  0.72541213,\n",
       "       -0.582896  ,  0.0717025 ,  0.09741304,  0.27031108,  0.16395958])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vectors[0] #avg w2v of first sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8f3a429fd62ae075d105a0a715216d8b18d5c77"
   },
   "source": [
    "## avg  TF-IDF W2V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d5e4dca6a2507a4b3129978a6f16a9f95bba488d"
   },
   "outputs": [],
   "source": [
    "# tfidf words/col names\n",
    "tfidf_feat = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "d97b495bba67a735d62d693681079491ce8b343e"
   },
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent[:20000]: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in vocab:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tf_idf = tf_idf_cleaned_text[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "099cb97eb1c6289d7bdd30b00463dc039e49a0a9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_sent_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
